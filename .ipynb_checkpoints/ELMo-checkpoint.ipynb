{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53906853-a114-4829-aa95-935e9c5d4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7d7d5-13a9-4ce6-b15e-7e3afc68030f",
   "metadata": {},
   "source": [
    "# ELMo Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba222037-356d-4017-91c0-51b488f61d34",
   "metadata": {},
   "source": [
    "The ELMo Language model constists of the following componenets:\n",
    "- The model inputs a sequence of words\n",
    "- Each word is broken down into their respective characters, where each caracter is then one-hot encoded to form a sparse matrix for every word.\n",
    "- Each matrix is then multiplied by a learned embedding matrix to reduce the dimentionality into a dense character matrix\n",
    "- The dense character matrix is then input into a CNN to generate a character-level representation of each of the words in the input sequence\n",
    "- The character-level representations are then input into the 2 bi-directional LSTM layers, to process the input sequence both forwards and backwards\n",
    "- The outputs for each layer is then concatinated and using to get a weighted average of the input embeddings\n",
    "- Final contextualized word embeddings are then obtained the weighted average of the input words with the original word embeddings (those generated with the learned embedding matrix)\n",
    "\n",
    "The use of the CNN over the character matrix allows the model to capture more nuanced relations between characters and words, as well as to be able to process out-of-vocabulary words. After this, the bidirectional language model allows the words in the input sequence to gain context from both directions, rather than just forward. Overall, these components allows the model to capture more complex linguistic phenomena and it has shown through various benchmarks to outperform almost all other existing models at the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c3712-098f-4fc0-bb23-1c7e2abca63d",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8636a3-056b-48ff-8caa-8c3789eaa8a7",
   "metadata": {},
   "source": [
    "In this notebook I will create a genre classifier model for poems in order to showcase ELMo using PyTorch. The dataset can be found [here](https://www.kaggle.com/datasets/ramjasmaurya/poem-classification-nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496c820-0ab6-42c4-88dc-3d57beb1d90c",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecef750-3298-45bf-ad7b-4b4506ff7f0c",
   "metadata": {},
   "source": [
    "The dataset that I am using for this project and analysis contains around 1000 poems, each being categorized into one of four genres: Affection, Environment, Music, and Death. The dataset consists of a total of 2 columns: the poem text its respective genre. The poems and their genre were obtained by scraping the website www.poets.org using the BeautifulSoup python library. There is potential to continue this effort to scrape possibly more data and expand the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c30b806-516d-467c-8f53-1f5a0af84aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n",
      "========\n",
      "   Genre  \\\n",
      "0  Music   \n",
      "1  Music   \n",
      "2  Music   \n",
      "3  Music   \n",
      "4  Music   \n",
      "\n",
      "                                                                                                                                                                                                      Poem  \n",
      "0                                                                                                                                                                                                      NaN  \n",
      "1                In the thick brushthey spend the hottest part of the day,              soaking their hoovesin the trickle of mountain water              the ravine hoardson behalf of the oleander.  ...  \n",
      "2             Storms are generous.                                      Something so easy to surrender to, sitting by the window, and then you step out into the garden you were so bored of,               \n",
      "3   —After Ana Mendieta Did you carry around the matin star? Did you hold forest-fire in one hand? Would you wake to radiate, shimmer, gleam lucero-light? Through the morning would you measure the wi...  \n",
      "4   for Aja Sherrard at 20The portent may itself be memory. —Wallace StevensHow hard to carry scores of adults on your back,not look at them as carrions of need, the distressof what loyalty requires....  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_train = pd.read_csv(\"data/poem_train.csv\")\n",
    "df_test = pd.read_csv(\"data/poem_test.csv\")\n",
    "\n",
    "print(df_test.shape)\n",
    "print(\"========\")\n",
    "print(df_train.head())\n",
    "# sns.countplot(train['Genre'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eef9b37b-ac1d-4562-a161-a57b9300161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 841 entries, 0 to 840\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Genre   841 non-null    object\n",
      " 1   Poem    837 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 13.3+ KB\n",
      "None\n",
      "\n",
      "Testing Data: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Genre   150 non-null    object\n",
      " 1   Poem    150 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data: \\n\")\n",
    "print(df_train.info(), end=\"\\n\\n\")\n",
    "print(\"Testing Data: \\n\")\n",
    "print(df_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa975cdd-762a-4574-97e7-92eba089e973",
   "metadata": {},
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a0bd6-b084-4125-82fd-3207d4cbea00",
   "metadata": {},
   "source": [
    "There seem to be 4 null values within the training dataset. I decided to just drop the NaN instances from the training set, as there is a small amount, and there aren't any ways I know of to impute extended text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "236e7a55-422e-4ca8-b383-bf2ab277570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Death</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Genre Poem\n",
       "count       4    0\n",
       "unique      2    0\n",
       "top     Death  NaN\n",
       "freq        3  NaN"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"{:.2%}\".format(len(df_train[df_train[\"Poem\"].isna()])/len(df_train)))\n",
    "df_train[df_train[\"Poem\"].isna()].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8863faf-af94-4c09-a86d-1217ff485799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train[\"Poem\"].isna() == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d446ce-ba50-4e40-a3cb-f232044e931b",
   "metadata": {},
   "source": [
    "Now lets check for duplicate values, just in case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9529db4-e573-4d5e-9d3b-6cce19ac01cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diplicates in training set:  0\n",
      "Duplicates in test set:  0\n"
     ]
    }
   ],
   "source": [
    "idx = df_train.duplicated()\n",
    "print(\"Diplicates in training set: \", len(df_train[idx]))\n",
    "df_train = df_train[-idx]\n",
    "\n",
    "idx = df_test.duplicated()\n",
    "print(\"Duplicates in test set: \", len(df_test[idx]))\n",
    "df_test = df_test[-idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286d680-6979-4299-ac2f-fe76ce6140e3",
   "metadata": {},
   "source": [
    "There were a couple of duplicates in the train set, to my surpise. This must have been a result of a small bug when scraping the data. Either way, I just removed the instances. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
